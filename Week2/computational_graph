digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139951244859152 [label="
 (1, 11)" fillcolor=darkolivegreen1]
	139951244840432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 300)
mat1_sym_strides:       (300, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (300, 11)
mat2_sym_strides:       (1, 300)"]
	139951244840720 -> 139951244840432
	139951461660160 [label="output_layer.bias
 (11)" fillcolor=lightblue]
	139951461660160 -> 139951244840720
	139951244840720 [label=AccumulateGrad]
	139951244840624 -> 139951244840432
	139951244840624 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	139951244840576 -> 139951244840624
	139951244840576 [label="NativeBatchNormBackward0
----------------------------
eps         :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :          False
weight      : [saved tensor]"]
	139951244840864 -> 139951244840576
	139951244840864 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (1, 150528)
mat1_sym_strides:             ()
mat2            :           None
mat2_sym_sizes  :  (150528, 300)
mat2_sym_strides:    (1, 150528)"]
	139951244841056 -> 139951244840864
	139951249234960 [label="layers.0.0.bias
 (300)" fillcolor=lightblue]
	139951249234960 -> 139951244841056
	139951244841056 [label=AccumulateGrad]
	139951244841008 -> 139951244840864
	139951244841008 [label=TBackward0]
	139951244841104 -> 139951244841008
	139951249235040 [label="layers.0.0.weight
 (300, 150528)" fillcolor=lightblue]
	139951249235040 -> 139951244841104
	139951244841104 [label=AccumulateGrad]
	139951244840816 -> 139951244840576
	139951461657200 [label="layers.0.1.weight
 (300)" fillcolor=lightblue]
	139951461657200 -> 139951244840816
	139951244840816 [label=AccumulateGrad]
	139951244840528 -> 139951244840576
	139951463390544 [label="layers.0.1.bias
 (300)" fillcolor=lightblue]
	139951463390544 -> 139951244840528
	139951244840528 [label=AccumulateGrad]
	139951244840672 -> 139951244840432
	139951244840672 [label=TBackward0]
	139951244840960 -> 139951244840672
	139951462570144 [label="output_layer.weight
 (11, 300)" fillcolor=lightblue]
	139951462570144 -> 139951244840960
	139951244840960 [label=AccumulateGrad]
	139951244840432 -> 139951244859152
}
